% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/elevate.R
\name{elevate}
\alias{elevate}
\title{Tune, Train, and Test an \pkg{rtemis} Learner by Nested Resampling}
\usage{
elevate(
  x,
  y = NULL,
  mod = "ranger",
  mod.params = list(),
  .preprocess = NULL,
  .decompose = NULL,
  weights = NULL,
  n.repeats = 1,
  outer.resampling = rtset.resample(resampler = "strat.sub", n.resamples = 10),
  inner.resampling = rtset.resample(resampler = "kfold", n.resamples = 5),
  bag.fn = median,
  x.name = NULL,
  y.name = NULL,
  save.mods = TRUE,
  save.tune = TRUE,
  bag.fitted = FALSE,
  outer.n.workers = 1,
  print.plot = TRUE,
  plot.fitted = FALSE,
  plot.predicted = TRUE,
  plot.theme = rtTheme,
  print.res.plot = FALSE,
  question = NULL,
  verbose = TRUE,
  res.verbose = FALSE,
  trace = 0,
  headless = FALSE,
  outdir = NULL,
  save.plots = FALSE,
  save.rt = ifelse(!is.null(outdir), TRUE, FALSE),
  save.mod = TRUE,
  save.res = FALSE,
  debug = FALSE,
  ...
)
}
\arguments{
\item{x}{Numeric vector or matrix / data frame of features i.e. independent variables}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{mod}{Character: Learner to use. Options: \link{modSelect}}

\item{mod.params}{Optional named list of parameters to be passed to \code{mod}. All parameters can
be passed as part of \code{...} as well}

\item{.preprocess}{Optional named list of parameters to be passed to \link{preprocess}. Set using
\link{rtset.preprocess}, e.g. \code{decom = rtset.preprocess(impute = TRUE)}}

\item{.decompose}{Optional named list of parameters to be used for decomposition / dimensionality
reduction. Set using \link{rtset.decompose}, e.g. \code{decom = rtset.decompose("ica", 12)}}

\item{weights}{Numeric vector: Weights for cases. For classification, \code{weights} takes precedence
over \code{ipw}, therefore set \code{weights = NULL} if using \code{ipw}.
Note: If \code{weight} are provided, \code{ipw} is not used. Leave NULL if setting \code{ipw = TRUE}. Default = NULL}

\item{bag.fn}{Function to use to average prediction if \code{bag.fitted = TRUE}. Default = \code{median}}

\item{x.name}{Character: Name of predictor dataset}

\item{y.name}{Character: Name of outcome}

\item{save.mods}{Logical: If TRUE, retain trained models in object, otherwise discard (save space
if running many resamples). Default = TRUE}

\item{save.tune}{Logical: If TRUE, save the best.tune data frame for each resample (output of gridSearchLearn)}

\item{bag.fitted}{Logical: If TRUE, use all models to also get a bagged prediction on the full sample. To get a
bagged prediction on new data using the same models, use \link{predict.rtModCV}}

\item{outer.n.workers}{Integer: Number of cores to use for the outer i.e. 
testing resamples. You are likely parallelizing either in the inner
(tuning) or the learner itself is parallelized. Don't parallelize the 
parallelization}

\item{print.plot}{Logical: if TRUE, produce plot using \code{mplot3}
Takes precedence over \code{plot.fitted} and \code{plot.predicted}. Default = TRUE}

\item{plot.fitted}{Logical: if TRUE, plot True (y) vs Fitted}

\item{plot.predicted}{Logical: if TRUE, plot True (y.test) vs Predicted.
Requires \code{x.test} and \code{y.test}}

\item{plot.theme}{Character: "zero", "dark", "box", "darkbox"}

\item{print.res.plot}{Logical: Print model performance plot for each resample.
Defaults to FALSE
from all resamples. Defaults to TRUE}

\item{question}{Character: the question you are attempting to answer with this model, in plain language.}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{res.verbose}{Logical: Passed to \link{resLearn_future}, passed to each individual learner's \code{verbose} argument}

\item{trace}{Integer: (Not really used) Print additional information if > 0. Default = 0}

\item{headless}{Logical: If TRUE, turn off all plotting.}

\item{outdir}{Character: Path where output should be saved}

\item{save.plots}{Logical: If TRUE, save plots to outdir}

\item{save.rt}{Logical: If TRUE and \code{outdir} is set, save all models to \code{outdir}}

\item{save.mod}{Logical: If TRUE, save all output to an RDS file in \code{outdir}
\code{save.mod} is TRUE by default if an \code{outdir} is defined. If set to TRUE, and no \code{outdir}
is defined, outdir defaults to \code{paste0("./s.", mod.name)}}

\item{save.res}{Logical: If TRUE, save the full output of each model trained on differents resamples under
subdirectories of \code{outdir}}

\item{debug}{Logical: If TRUE, sets \code{outer.n.workers} to 1, and \code{options(error=recover)}}

\item{...}{Additional mod.params to be passed to learner (Will be concatenated with \code{mod.params}, so that you can use
either way to pass learner arguments)}

\item{parallel.type}{Character: "psock" (Default), "fork"}
}
\value{
Object of class \code{rtModCV} (Regression) or \code{rtModCVClass} (Classification)
\item{error.test.repeats}{the mean or aggregate error, as appropriate, for each repeat}
\item{error.test.repeats.mean}{the mean error of all repeats, i.e. the mean of \code{error.test.repeats}}
\item{error.test.repeats.sd}{if \code{n.repeats} > 1, the standard deviation of \code{error.test.repeats}}
\item{error.test.res}{the error for each resample, for each repeat}
}
\description{
\code{elevate} is a high-level function to tune, train, and test an \pkg{rtemis} model
by nested resampling, with optional preprocessing and decomposition of input features
}
\details{
- Note on resampling: You can never use an outer resampling method with replacement
if you will also be using an inner resampling (for tuning).
The duplicated cases from the outer resampling may appear both in the training and
testing sets of the inner resamples, leading to artificially decreased error.

- If there is an error while running either the outer or inner resamples in parallel, the error
message returned by R will likely be unhelpful. Repeat the command after setting both inner
and outer resample run to use a single core, which should provide an informative message.
}
\examples{
\dontrun{
# Regression

x <- rnormmat(100, 50)
w <- rnorm(50)
y <- x \%*\% w + rnorm(50)
mod <- elevate(x, y)

# Classification

data(Sonar, package = "mlbench")
mod <- elevate(Sonar)

# Example usage of debug in elevate

# Train on a resample which has no cases for one level
ir <- iris[1:100, ]

# ranger works, but read those warnings!
mod <- elevate(ir)

# rpart fails but you can't tell what's going on
mod <- elevate(ir, mod = "cart")

# Enabling debug helps you find out what's going on where
mod <- elevate(ir, mod = "cart", debug = TRUE)
}
}
\author{
E.D. Gennatas
}
