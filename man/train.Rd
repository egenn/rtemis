% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\name{train}
\alias{train}
\title{Train Supervised Learning Models}
\usage{
train(
  x,
  dat_validation = NULL,
  dat_test = NULL,
  algorithm = NULL,
  preprocessor_parameters = NULL,
  hyperparameters = NULL,
  tuner_parameters = NULL,
  outer_resampling = NULL,
  weights = NULL,
  question = NULL,
  outdir = NULL,
  parallel_type = c("future", "mirai", "none"),
  future_plan = getOption("future.plan", "multicore"),
  n_workers = max(future::availableCores() - 3L, 1L),
  verbosity = 1L
)
}
\arguments{
\item{x}{data.frame or similar: Training set data.}

\item{dat_validation}{data.frame or similar: Validation set data.}

\item{dat_test}{data.frame or similar: Test set data.}

\item{algorithm}{Character: Algorithm to use. Can be left NULL, if \code{hyperparameters} is defined.}

\item{preprocessor_parameters}{PreprocessorParameters object or NULL: Setup using \link{setup_Preprocessor}.}

\item{hyperparameters}{Hyperparameters object: Setup using one of \verb{setup_*} functions.}

\item{tuner_parameters}{TunerParameters object: Setup using \link{setup_GridSearch}.}

\item{outer_resampling}{ResamplerParameters object or NULL: Setup using \link{setup_Resampler}. This
defines the outer resampling method, i.e. the splitting into training and test sets for the
purpose of assessing model performance. If NULL, no outer resampling is performed, in which case
you might want to use a \code{dat_test} dataset to assess model performance on a single test set.}

\item{weights}{Optional vector of case weights.}

\item{question}{Optional character string defining the question that the model is trying to
answer.}

\item{outdir}{Character, optional: String defining the output directory.}

\item{parallel_type}{Character: "none", "future", or "mirai".}

\item{future_plan}{Character: Future plan to use for parallel processing.}

\item{n_workers}{Integer: Number of workers to use for parallel processing in total.
Parallelization may happen at three different levels, from innermost to outermost:
\enumerate{
\item Algorithm training (e.g. a parallelized learner like LightGBM)
\item Tuning (inner resampling, where multiple resamples can be processed in parallel)
\item Outer resampling (where multiple outer resamples can be processed in parallel)
The \code{train()} function will assign the number of workers to the innermost available
parallelization level. Best to leave a few cores for the OS and other processes, especially
on shared systems or when working with large datasets, since parallelization will increase
memory usage.
}}

\item{verbosity}{Integer: Verbosity level.
\code{hyperparameters} is not defined. Avoid relying on this, instead use the appropriate \verb{setup_*}
function with the \code{hyperparameters} argument.}
}
\value{
Object of class \code{Regression(Supervised)}, \code{RegressionRes(SupervisedRes)},
\code{Classification(Supervised)}, or \code{ClassificationRes(SupervisedRes)}.
}
\description{
Preprocess, tune, train, and test supervised learning models with a single function
using nested resampling
}
\details{
Important: For binary classification, the outcome should be a factor where the 2nd level
corresponds to the positive class.

Note on resampling: You should never use an outer resampling method with
replacement if you will also be using an inner resampling (for tuning).
The duplicated cases from the outer resampling may appear both in the
training and test sets of the inner resamples, leading to underestimated
test error.
}
\author{
EDG
}
