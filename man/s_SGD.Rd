% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/s_SGD.R
\name{s_SGD}
\alias{s_SGD}
\title{Stochastic Gradient Descent (SGD) (C, R)}
\usage{
s_SGD(
  x,
  y = NULL,
  x.test = NULL,
  y.test = NULL,
  x.name = NULL,
  y.name = NULL,
  model = NULL,
  model.control = list(lambda1 = 0, lambda2 = 0),
  sgd.control = list(method = "ai-sgd"),
  upsample = FALSE,
  downsample = FALSE,
  resample.seed = NULL,
  print.plot = FALSE,
  plot.fitted = NULL,
  plot.predicted = NULL,
  plot.theme = rtTheme,
  question = NULL,
  verbose = TRUE,
  outdir = NULL,
  save.mod = ifelse(!is.null(outdir), TRUE, FALSE),
  ...
)
}
\arguments{
\item{x}{Numeric vector or matrix / data frame of features i.e. independent variables}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{x.test}{Numeric vector or matrix / data frame of testing set features
Columns must correspond to columns in \code{x}}

\item{y.test}{Numeric vector of testing set outcome}

\item{x.name}{Character: Name for feature set}

\item{y.name}{Character: Name for outcome}

\item{model}{character specifying the model to be used: \code{"lm"} (linear
model), \code{"glm"} (generalized linear model), \code{"cox"} (Cox
proportional hazards model), \code{"gmm"} (generalized method of moments),
\code{"m"} (M-estimation). See \sQuote{Details}.}

\item{model.control}{a list of parameters for controlling the model.
\describe{
  \item{\code{family} (\code{"glm"})}{a description of the error distribution and
    link function to be used in the model. This can be a character string
    naming a family function, a family function or the result of a call to
    a family function. (See \code{\link[stats]{family}} for details of
    family functions.)}
  \item{\code{rank} (\code{"glm"})}{logical. Should the rank of the design matrix
    be checked?}
  \item{\code{fn} (\code{"gmm"})}{a function \eqn{g(\theta,x)} which returns a
    \eqn{k}-vector corresponding to the \eqn{k} moment conditions. It is a
    required argument if \code{gr} not specified.}
  \item{\code{gr} (\code{"gmm"})}{a function to return the gradient. If
    unspecified, a finite-difference approximation will be used.}
  \item{\code{nparams} (\code{"gmm"})}{number of model parameters. This is
    automatically determined for other models.}
  \item{\code{type} (\code{"gmm"})}{character specifying the generalized method of
    moments procedure: \code{"twostep"} (Hansen, 1982), \code{"iterative"}
    (Hansen et al., 1996). Defaults to \code{"iterative"}.}
  \item{\code{wmatrix} (\code{"gmm"})}{weighting matrix to be used in the loss
    function. Defaults to the identity matrix.}
  \item{\code{loss} (\code{"m"})}{character specifying the loss function to be
    used in the estimating equation. Default is the Huber loss.}
  \item{\code{lambda1}}{L1 regularization parameter. Default is 0.}
  \item{\code{lambda2}}{L2 regularization parameter. Default is 0.}
}}

\item{sgd.control}{an optional list of parameters for controlling the estimation.
\describe{
  \item{\code{method}}{character specifying the method to be used: \code{"sgd"},
    \code{"implicit"}, \code{"asgd"}, \code{"ai-sgd"}, \code{"momentum"},
    \code{"nesterov"}. Default is \code{"ai-sgd"}. See \sQuote{Details}.}
  \item{\code{lr}}{character specifying the learning rate to be used:
    \code{"one-dim"}, \code{"one-dim-eigen"}, \code{"d-dim"},
    \code{"adagrad"}, \code{"rmsprop"}. Default is \code{"one-dim"}.
    See \sQuote{Details}.}
  \item{\code{lr.control}}{vector of scalar hyperparameters one can
    set dependent on the learning rate. For hyperparameters aimed
    to be left as default, specify \code{NA} in the corresponding
    entries. See \sQuote{Details}.}
  \item{\code{start}}{starting values for the parameter estimates. Default is
    random initialization around zero.}
  \item{\code{size}}{number of SGD estimates to store for diagnostic purposes
    (distributed log-uniformly over total number of iterations)}
  \item{\code{reltol}}{relative convergence tolerance. The algorithm stops
    if it is unable to change the relative mean squared difference in the
    parameters by more than the amount. Default is \code{1e-05}.}
  \item{\code{npasses}}{the maximum number of passes over the data. Default
    is 3.}
  \item{\code{pass}}{logical. Should \code{tol} be ignored and run the
    algorithm for all of \code{npasses}?}
  \item{\code{shuffle}}{logical. Should the algorithm shuffle the data set
    including for each pass?}
  \item{\code{verbose}}{logical. Should the algorithm print progress?}
}}

\item{upsample}{Logical: If TRUE, upsample cases to balance outcome classes (for Classification only)
Note: upsample will randomly sample with replacement if the length of the majority class is more than double
the length of the class you are upsampling, thereby introducing randomness}

\item{downsample}{Logical: If TRUE, downsample majority class to match size of minority class}

\item{resample.seed}{Integer: If provided, will be used to set the seed during upsampling.
Default = NULL (random seed)}

\item{print.plot}{Logical: if TRUE, produce plot using \code{mplot3}
Takes precedence over \code{plot.fitted} and \code{plot.predicted}.}

\item{plot.fitted}{Logical: if TRUE, plot True (y) vs Fitted}

\item{plot.predicted}{Logical: if TRUE, plot True (y.test) vs Predicted.
Requires \code{x.test} and \code{y.test}}

\item{plot.theme}{Character: "zero", "dark", "box", "darkbox"}

\item{question}{Character: the question you are attempting to answer with this model, in plain language.}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{outdir}{Path to output directory.
If defined, will save Predicted vs. True plot, if available,
as well as full model output, if \code{save.mod} is TRUE}

\item{save.mod}{Logical: If TRUE, save all output to an RDS file in \code{outdir}
\code{save.mod} is TRUE by default if an \code{outdir} is defined. If set to TRUE, and no \code{outdir}
is defined, outdir defaults to \code{paste0("./s.", mod.name)}}

\item{...}{Additional arguments to be passed to \code{sgd.control}}
}
\value{
Object of class \pkg{rtemis}
}
\description{
Train a model by Stochastic Gradient Descent using \code{sgd::sgd}
}
\details{
From \code{sgd::sgd}:
"Models: The Cox model assumes that the survival data is ordered when passed in, i.e.,
such that the risk set of an observation i is all data points after it."
}
\seealso{
\link{train_cv} for external cross-validation

Other Supervised Learning: 
\code{\link{s_AdaBoost}()},
\code{\link{s_AddTree}()},
\code{\link{s_BART}()},
\code{\link{s_BRUTO}()},
\code{\link{s_BayesGLM}()},
\code{\link{s_C50}()},
\code{\link{s_CART}()},
\code{\link{s_CTree}()},
\code{\link{s_EVTree}()},
\code{\link{s_GAM}()},
\code{\link{s_GBM}()},
\code{\link{s_GLM}()},
\code{\link{s_GLMNET}()},
\code{\link{s_GLMTree}()},
\code{\link{s_GLS}()},
\code{\link{s_H2ODL}()},
\code{\link{s_H2OGBM}()},
\code{\link{s_H2ORF}()},
\code{\link{s_HAL}()},
\code{\link{s_Isotonic}()},
\code{\link{s_KNN}()},
\code{\link{s_LDA}()},
\code{\link{s_LM}()},
\code{\link{s_LMTree}()},
\code{\link{s_LightCART}()},
\code{\link{s_LightGBM}()},
\code{\link{s_MARS}()},
\code{\link{s_MLRF}()},
\code{\link{s_NBayes}()},
\code{\link{s_NLA}()},
\code{\link{s_NLS}()},
\code{\link{s_NW}()},
\code{\link{s_PPR}()},
\code{\link{s_PolyMARS}()},
\code{\link{s_QDA}()},
\code{\link{s_QRNN}()},
\code{\link{s_RF}()},
\code{\link{s_RFSRC}()},
\code{\link{s_Ranger}()},
\code{\link{s_SDA}()},
\code{\link{s_SPLS}()},
\code{\link{s_SVM}()},
\code{\link{s_TFN}()},
\code{\link{s_XGBoost}()},
\code{\link{s_XRF}()}
}
\author{
E.D. Gennatas
}
\concept{Supervised Learning}
