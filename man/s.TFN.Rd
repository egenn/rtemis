% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/s.TFN.R
\name{s.TFN}
\alias{s.TFN}
\title{Neural Network with \pkg{tensorflow} [C, R]}
\usage{
s.TFN(x, y = NULL, x.test = NULL, y.test = NULL, x.valid = NULL,
  y.valid = NULL, upsample = FALSE, upsample.seed = NULL,
  net = NULL, n.hidden.nodes = NULL,
  initializer = c("glorot_uniform", "glorot_normal", "he_uniform",
  "he_normal", "lecun_uniform", "lecun_normal", "random_uniform",
  "random_normal", "variance_scaling", "truncated_normal", "orthogonal",
  "zeros", "ones", "constant"), initializer.seed = NULL, dropout = 0,
  activation = c("relu", "selu", "elu", "sigmoid", "hard_sigmoid",
  "tanh", "exponential", "linear", "softmax", "softplus", "softsign"),
  l1 = 0, l2 = 0, batch.normalization = TRUE, output = NULL,
  loss = NULL, optimizer = c("rmsprop", "adadelta", "adagrad", "adam",
  "adamax", "nadam", "sgd"), learning.rate = NULL, metric = NULL,
  epochs = 50, batch.size = NULL, validation.split = 0.2,
  callback = keras::callback_early_stopping(patience = 150),
  scale = TRUE, x.name = NULL, y.name = NULL, print.plot = TRUE,
  print.error.plot = NULL, rtlayout.mat = c(2, 1),
  plot.fitted = NULL, plot.predicted = NULL,
  plot.theme = getOption("rt.fit.theme", "lightgrid"), question = NULL,
  verbose = TRUE, verbose.checkpoint = FALSE, outdir = NULL,
  save.mod = ifelse(!is.null(outdir), TRUE, FALSE), ...)
}
\arguments{
\item{x}{Numeric vector or matrix / data frame of features i.e. independent variables}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{x.test}{Numeric vector or matrix / data frame of testing set features
Columns must correspond to columns in \code{x}}

\item{y.test}{Numeric vector of testing set outcome}

\item{upsample}{Logical: If TRUE, upsample cases to balance outcome classes (for Classification only)
Caution: upsample will randomly sample with replacement if the length of the majority class is more than double
the length of the class you are upsampling, thereby introducing randomness}

\item{upsample.seed}{Integer: If provided, will be used to set the seed during upsampling.
Default = NULL (random seed)}

\item{n.hidden.nodes}{Integer vector: Length must be equal to the number of hidden layers you wish to create.
Can be zero (~GLM)}

\item{initializer}{String: Initializer to use for each layer: "glorot_uniform", "glorot_normal", "he_uniform",
"he_normal", "cun_uniform", "lecun_normal", "random_uniform", "random_normal", "variance_scaling",
"truncated_normal", "orthogonal", "zeros", "ones", "constant".
Glorot is also known as Xavier initialization. Default = "glorot_uniform"}

\item{initializer.seed}{Integer: Seed to use for each initializer for reproducibility. Default = NULL}

\item{dropout}{Floar, vector, (0, 1): Probability of dropping nodes. Can be a vector of length equal to N of layers,
otherwise will be recycled. Default = 0}

\item{activation}{String vector: Activation type to use: "relu", "selu", "elu", "sigmoid", "hard_sigmoid", "tanh",
"exponential", "linear", "softmax", "softplus", "softsign". Default = "relu"}

\item{batch.normalization}{Logical: If TRUE, batch normalize after each hidden layer. Default = TRUE}

\item{output}{String: Activation to use for output layer. Can be any as in \code{activation}.
Default = "linear" for Regression, "sigmoid" for binary classification, "softmax" for multiclass}

\item{loss}{String: Loss to use: Default = "mean_squared_error" for regression, "binary_crossentropy" for binary
classification, "sparse_categorical_crossentropy" for multiclass}

\item{optimizer}{String: Optimization to use: "rmsprop", "adadelta", "adagrad", "adam", "adamax", "nadam", "sgd".
Default = "rmsprop"}

\item{learning.rate}{Float: learning rate. Defaults depend on \code{optimizer} used and are:
\code{rmsprop = .001, adadelta = 1, adagrad = .01, adamax = .002, adam = .001, nadam = .002, sgd = .1}}

\item{metric}{String: Metric used for evaluation during train. Default = "mse" for regression,
"accuracy" for classification.}

\item{epochs}{Integer: Number of epochs. Default = 100}

\item{batch.size}{Integer: Batch size. Default = N of cases}

\item{validation.split}{Float (0, 1): proportion of training data to use for validation. Default = .2}

\item{callback}{Function to be called by keras during fitting.
Default = \code{keras::callback_early_stopping(patience = 150)} for early stopping.}

\item{scale}{Logical: If TRUE, scale featues before training. Default = TRUE
column means and standard deviation will be saved in \code{rtMod$extra} field to allow
scaling ahead of prediction on new data}

\item{x.name}{Character: Name for feature set}

\item{y.name}{Character: Name for outcome}

\item{print.plot}{Logical: if TRUE, produce plot using \code{mplot3}
Takes precedence over \code{plot.fitted} and \code{plot.predicted}}

\item{plot.fitted}{Logical: if TRUE, plot True (y) vs Fitted}

\item{plot.predicted}{Logical: if TRUE, plot True (y.test) vs Predicted.
Requires \code{x.test} and \code{y.test}}

\item{plot.theme}{String: "zero", "dark", "box", "darkbox"}

\item{question}{String: the question you are attempting to answer with this model, in plain language.}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{outdir}{Path to output directory.
If defined, will save Predicted vs. True plot, if available,
as well as full model output, if \code{save.mod} is TRUE}

\item{save.mod}{Logical. If TRUE, save all output as RDS file in \code{outdir}
\code{save.mod} is TRUE by default if an \code{outdir} is defined. If set to TRUE, and no \code{outdir}
is defined, outdir defaults to \code{paste0("./s.", mod.name)}}

\item{...}{Additional parameters}
}
\description{
Train a Neural Network using \pkg{keras} and \pkg{tensorflow}
}
\details{
For more information on argument and hyperparameters, see (https://keras.rstudio.com/) and (https://keras.io/)
It is important to define network structure and adjust hyperparameters. You cannot expect defaults to work
on any dataset.
}
\seealso{
\link{elevate} for external cross-validation

Other Supervised Learning: \code{\link{s.ADABOOST}},
  \code{\link{s.ADDTREE}}, \code{\link{s.BART}},
  \code{\link{s.BAYESGLM}}, \code{\link{s.BRUTO}},
  \code{\link{s.C50}}, \code{\link{s.CART}},
  \code{\link{s.CTREE}}, \code{\link{s.DA}},
  \code{\link{s.ET}}, \code{\link{s.EVTREE}},
  \code{\link{s.GAM.default}}, \code{\link{s.GAM.formula}},
  \code{\link{s.GAMSEL}}, \code{\link{s.GAM}},
  \code{\link{s.GBM3}}, \code{\link{s.GBM}},
  \code{\link{s.GLMNET}}, \code{\link{s.GLM}},
  \code{\link{s.GLS}}, \code{\link{s.H2ODL}},
  \code{\link{s.H2OGBM}}, \code{\link{s.H2ORF}},
  \code{\link{s.IRF}}, \code{\link{s.KNN}},
  \code{\link{s.LDA}}, \code{\link{s.LM}},
  \code{\link{s.MARS}}, \code{\link{s.MLRF}},
  \code{\link{s.MXN}}, \code{\link{s.NBAYES}},
  \code{\link{s.NLA}}, \code{\link{s.NLS}},
  \code{\link{s.NW}}, \code{\link{s.POLYMARS}},
  \code{\link{s.PPR}}, \code{\link{s.PPTREE}},
  \code{\link{s.QDA}}, \code{\link{s.QRNN}},
  \code{\link{s.RANGER}}, \code{\link{s.RFSRC}},
  \code{\link{s.RF}}, \code{\link{s.SGD}},
  \code{\link{s.SPLS}}, \code{\link{s.SVM}},
  \code{\link{s.XGBLIN}}, \code{\link{s.XGB}}

Other Deep Learning: \code{\link{d.H2OAE}},
  \code{\link{p.MXINCEPTION}}, \code{\link{s.H2ODL}},
  \code{\link{s.MXN}}
}
\author{
Efstathios D. Gennatas
}
\concept{Deep Learning}
\concept{Supervised Learning}
