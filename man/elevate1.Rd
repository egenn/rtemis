% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/elevate1.R
\name{elevate1}
\alias{elevate1}
\title{Tune, Train, and Test an \pkg{rtemis} Learner by Nested Resampling}
\usage{
elevate1(
  x,
  y = NULL,
  mod = "ranger",
  mod.params = list(),
  .preprocess = NULL,
  .decompose = NULL,
  .resample = NULL,
  weights = NULL,
  resampler = "strat.sub",
  n.resamples = 10,
  n.repeats = 1,
  stratify.var = NULL,
  train.p = 0.8,
  strat.n.bins = 4,
  target.length = NULL,
  seed = NULL,
  res.index = NULL,
  res.group = NULL,
  bag.fn = median,
  x.name = NULL,
  y.name = NULL,
  save.mods = TRUE,
  save.tune = TRUE,
  bag.fitted = FALSE,
  outer.n.workers = 1,
  parallel.type = ifelse(.Platform$OS.type == "unix", "fork", "psock"),
  print.plot = TRUE,
  plot.fitted = FALSE,
  plot.predicted = TRUE,
  plot.theme = rtTheme,
  print.res.plot = FALSE,
  question = NULL,
  verbose = TRUE,
  res.verbose = FALSE,
  trace = 0,
  headless = FALSE,
  outdir = NULL,
  save.plots = FALSE,
  save.rt = ifelse(!is.null(outdir), TRUE, FALSE),
  save.mod = TRUE,
  save.res = FALSE,
  backend = "future",
  debug = FALSE,
  ...
)
}
\arguments{
\item{x}{Numeric vector or matrix / data frame of features i.e. independent variables}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{mod}{Character: Learner to use. Options: \link{modSelect}}

\item{mod.params}{Optional named list of parameters to be passed to \code{mod}. All parameters can
be passed as part of \code{...} as well}

\item{.preprocess}{Optional named list of parameters to be passed to \link{preprocess}. Set using
\link{rtset.preprocess}, e.g. \code{decom = rtset.preprocess(impute = TRUE)}}

\item{.decompose}{Optional named list of parameters to be used for decomposition / dimensionality
reduction. Set using \link{rtset.decompose}, e.g. \code{decom = rtset.decompose("ica", 12)}}

\item{.resample}{Optional named list of parameters to be passed to \link{resample}.
NOTE: If set, this takes precedence over setting the individual resampling arguments}

\item{weights}{Numeric vector: Weights for cases. For classification, \code{weights} takes precedence
over \code{ipw}, therefore set \code{weights = NULL} if using \code{ipw}.
Note: If \code{weight} are provided, \code{ipw} is not used. Leave NULL if setting \code{ipw = TRUE}. Default = NULL}

\item{resampler}{Character: Type of resampling to perform: "bootstrap", "kfold",
"strat.boot", "strat.sub".}

\item{n.resamples}{Integer: Number of training/testing sets required}

\item{n.repeats}{Integer: Number of times the external resample should be repeated. This allows you to do,
for example, 10 times 10-fold crossvalidation. Default = 1. In most cases it makes sense to use 1 repeat of
many resamples, e.g. 25 stratified subsamples,}

\item{stratify.var}{Numeric vector: Used to stratify external sampling (if applicable)
Defaults to outcome \code{y}}

\item{train.p}{Float (0, 1): Fraction of cases to assign to traininig set for
\code{resampler = "strat.sub"}}

\item{strat.n.bins}{Integer: Number of groups to use for stratification for
\code{resampler = "strat.sub" / "strat.boot"}}

\item{target.length}{Integer: Number of cases for training set for
\code{resampler = "strat.boot"}.}

\item{seed}{Integer: (Optional) Set seed for random number generator, in order to make
output reproducible. See \code{?base::set.seed}}

\item{res.index}{List where each element is a vector of training set indices. Use this for manual or
precalculated train/test splits}

\item{res.group}{Integer, vector, length = length(y): Integer vector, where numbers define fold membership.
e.g. for 10-fold on a dataset with 1000 cases, you could use group = rep(1:10, each = 100)}

\item{bag.fn}{Function to use to average prediction if \code{bag.fitted = TRUE}. Default = \code{median}}

\item{x.name}{Character: Name of predictor dataset}

\item{y.name}{Character: Name of outcome}

\item{save.mods}{Logical: If TRUE, retain trained models in object, otherwise discard (save space
if running many resamples). Default = TRUE}

\item{save.tune}{Logical: If TRUE, save the best.tune data frame for each resample (output of gridSearchLearn)}

\item{bag.fitted}{Logical: If TRUE, use all models to also get a bagged prediction on the full sample. To get a
bagged prediction on new data using the same models, use \link{predict.rtModCV}}

\item{outer.n.workers}{Integer: Number of cores to use. Default = 1. You are likely parallelizing either in the inner
(tuning) or the learner itself is parallelized. Don't parallelize the parallelization}

\item{parallel.type}{Character: "psock" (Default), "fork"}

\item{print.plot}{Logical: if TRUE, produce plot using \code{mplot3}
Takes precedence over \code{plot.fitted} and \code{plot.predicted}. Default = TRUE}

\item{plot.fitted}{Logical: if TRUE, plot True (y) vs Fitted}

\item{plot.predicted}{Logical: if TRUE, plot True (y.test) vs Predicted.
Requires \code{x.test} and \code{y.test}}

\item{plot.theme}{Character: "zero", "dark", "box", "darkbox"}

\item{print.res.plot}{Logical: Print model performance plot for each resample.
Defaults to FALSE
from all resamples. Defaults to TRUE}

\item{question}{Character: the question you are attempting to answer with this model, in plain language.}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{res.verbose}{Logical: Passed to \link{resLearn_future}, passed to each individual learner's \code{verbose} argument}

\item{trace}{Integer: (Not really used) Print additional information if > 0. Default = 0}

\item{headless}{Logical: If TRUE, turn off all plotting.}

\item{outdir}{Character: Path where output should be saved}

\item{save.plots}{Logical: If TRUE, save plots to outdir}

\item{save.rt}{Logical: If TRUE and \code{outdir} is set, save all models to \code{outdir}}

\item{save.mod}{Logical: If TRUE, save all output to an RDS file in \code{outdir}
\code{save.mod} is TRUE by default if an \code{outdir} is defined. If set to TRUE, and no \code{outdir}
is defined, outdir defaults to \code{paste0("./s.", mod.name)}}

\item{save.res}{Logical: If TRUE, save the full output of each model trained on differents resamples under
subdirectories of \code{outdir}}

\item{backend}{(For testing use only)}

\item{debug}{Logical: If TRUE, sets \code{outer.n.workers} to 1, and \code{options(error=recover)}}

\item{...}{Additional mod.params to be passed to learner (Will be concatenated with \code{mod.params}, so that you can use
either way to pass learner arguments)}
}
\value{
Object of class \code{rtModCV} (Regression) or \code{rtModCVClass} (Classification)
\item{error.test.repeats}{the mean or aggregate error, as appropriate, for each repeat}
\item{error.test.repeats.mean}{the mean error of all repeats, i.e. the mean of \code{error.test.repeats}}
\item{error.test.repeats.sd}{if \code{n.repeats} > 1, the standard deviation of \code{error.test.repeats}}
\item{error.test.res}{the error for each resample, for each repeat}
}
\description{
\code{elevate} is a high-level function to tune, train, and test an \pkg{rtemis} model
by nested resampling, with optional preprocessing and decomposition of input features
}
\details{
This is the old elevate function maintained here for compatibility with 
older projects.

- Note on resampling: You can never use an outer resampling method with replacement
if you will also be using an inner resampling (for tuning).
The duplicated cases from the outer resampling may appear both in the training and
testing sets of the inner resamples, leading to artificially decreased error.

- If there is an error while running either the outer or inner resamples in parallel, the error
message returned by R will likely be unhelpful. Repeat the command after setting both inner
and outer resample run to use a single core, which should provide an informative message.
}
\examples{
\dontrun{
# Regression

x <- rnormmat(100, 50)
w <- rnorm(50)
y <- x \%*\% w + rnorm(50)
mod <- elevate(x, y)

# Classification

data(Sonar, package = "mlbench")
mod <- elevate(Sonar)

# Example usage of debug in elevate

# Train on a resample which has no cases for one level
ir <- iris[1:100, ]

# ranger works, but read those warnings!
mod <- elevate(ir)

# rpart fails but you can't tell what's going on
mod <- elevate(ir, mod = "cart")

# Enabling debug helps you find out what's going on where
mod <- elevate(ir, mod = "cart", debug = TRUE)
}
}
\author{
E.D. Gennatas
}
