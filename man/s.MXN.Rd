% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/s.MXN.R
\name{s.MXN}
\alias{s.MXN}
\title{Neural Network with \code{mxnet} [C, R]}
\usage{
s.MXN(x, y = NULL, x.test = NULL, y.test = NULL, x.valid = NULL,
  y.valid = NULL, upsample = FALSE, downsample = FALSE,
  resample.seed = NULL, net = NULL, n.hidden.nodes = NULL,
  output = NULL, ctx = mxnet::mx.cpu(),
  initializer = mxnet::mx.init.Xavier(), batch.normalization = TRUE,
  l2.normalization = FALSE, activation = "relu",
  optimizer = "adadelta", batch.size = NULL, momentum = 0.9,
  max.epochs = 1000, min.epochs = 25, early.stop = c("train",
  "valid"), early.stop.absolute.threshold = NA,
  early.stop.relative.threshold = NA,
  early.stop.relativeVariance.threshold = NULL,
  early.stop.n.steps = NULL, learning.rate = NULL, dropout = 0,
  dropout.before = 1, dropout.after = 0, eval.metric = NULL,
  minimize = NULL, arg.params = NULL, mx.seed = NULL,
  x.name = NULL, y.name = NULL, plot.graphviz = FALSE,
  print.plot = TRUE, print.error.plot = NULL, rtlayout.mat = c(2, 1),
  plot.fitted = NULL, plot.predicted = NULL,
  plot.theme = getOption("rt.fit.theme", "lightgrid"), question = NULL,
  verbose = TRUE, verbose.mxnet = TRUE, verbose.checkpoint = FALSE,
  outdir = NULL, n.cores = rtCores,
  save.mod = ifelse(!is.null(outdir), TRUE, FALSE), ...)
}
\arguments{
\item{x}{Numeric vector or matrix / data frame of features i.e. independent variables}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{x.test}{Numeric vector or matrix / data frame of testing set features
Columns must correspond to columns in \code{x}}

\item{y.test}{Numeric vector of testing set outcome}

\item{upsample}{Logical: If TRUE, upsample cases to balance outcome classes (for Classification only)
Caution: upsample will randomly sample with replacement if the length of the majority class is more than double
the length of the class you are upsampling, thereby introducing randomness}

\item{resample.seed}{Integer: If provided, will be used to set the seed during upsampling.
Default = NULL (random seed)}

\item{net}{MXNET Symbol: provide a previously defined network. logger will not work in this case at the moment,
so early stopping cannot be applied}

\item{n.hidden.nodes}{Integer vector: Length must be equal to the number of hidden layers you wish to create}

\item{output}{String: "Logistic" for binary classification, "Softmax" for classification of 2 or more classes,
"Linear" for Regression. Defaults to "Logistic" for binary outcome, "Softmax" for 3+ classes, "LinearReg" for
regression.}

\item{ctx}{MXNET context: \code{mxnet::mx.cpu()} to use CPU(s). Define N of cores using \code{n.cores} argument.
\code{mxnet::mx.gpu()} to use GPU. For multiple GPUs, provide list like such:
\code{ctx = list(mxnet::mx.gpu(0), mxnet::mx.gpu(1)} to use two GPUs.}

\item{batch.normalization}{Logical: If TRUE, batch normalize before activation. Default = TRUE}

\item{l2.normalization}{Logical: If TRUE, apply L2 normalization after fully connected step. Default = FALSE}

\item{activation}{String vector: Activation types to use: 'relu', 'sigmoid', 'softrelu', 'tanh'.
If length < n of hidden layers, elements are recycled. See \code{mxnet::mx.symbol.Activation}}

\item{max.epochs}{Integer: Number of iterations for training.}

\item{learning.rate}{Float: learning rate}

\item{dropout}{Float (0, 1): Probability of dropping nodes}

\item{dropout.before}{Integer: Index of hidden layer before which dropout should be applied}

\item{dropout.after}{Integer: Index of hidden layer after which dropout should be applied}

\item{eval.metric}{String: Metrix used for evaluation during train. Default: "rmse"}

\item{x.name}{Character: Name for feature set}

\item{y.name}{Character: Name for outcome}

\item{plot.graphviz}{Logical: if TRUE, plot the network structure using \code{graphviz}}

\item{print.plot}{Logical: if TRUE, produce plot using \code{mplot3}
Takes precedence over \code{plot.fitted} and \code{plot.predicted}}

\item{plot.fitted}{Logical: if TRUE, plot True (y) vs Fitted}

\item{plot.predicted}{Logical: if TRUE, plot True (y.test) vs Predicted.
Requires \code{x.test} and \code{y.test}}

\item{plot.theme}{String: "zero", "dark", "box", "darkbox"}

\item{question}{String: the question you are attempting to answer with this model, in plain language.}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{outdir}{Path to output directory.
If defined, will save Predicted vs. True plot, if available,
as well as full model output, if \code{save.mod} is TRUE}

\item{n.cores}{Integer: Number of cores to use. Caution: Only set to >1 if you're sure MXNET is not using already
using multiple cores}

\item{save.mod}{Logical. If TRUE, save all output as RDS file in \code{outdir}
\code{save.mod} is TRUE by default if an \code{outdir} is defined. If set to TRUE, and no \code{outdir}
is defined, outdir defaults to \code{paste0("./s.", mod.name)}}

\item{...}{Additional parameters to be passed to \code{mxnet::mx.model.FeedForward.create}}
}
\description{
Train a Neural Network using \code{mxnet} with optional early stopping
}
\details{
Early stopping is considered after training has taken place for \code{min.epochs} epochs.
After that point, early stopping is controlled by three criteria:
an absolute threshold (\code{early.stop.absolute.threshold}),
a relative threshold (\code{early.stop.relative.threshold}),
or a relative variance across a set number of steps (\code{early.stop.realtiveVariance.threshold} along
\code{early.stop.n.steps}).
Early stopping by default (if you change none of the \code{early.stop} arguments), will look at training error
and stop when the relative variance of the loss over the last 24 steps (classification) or 12 steps (regression)
is lower than 5e-06 (classification) or lower than 5e-03 (regression). To set early stopping OFF, set all
early stopping criteria to NA.
It is important to tune learning rate and adjust max.epochs accordingly depending on the learning type
(Classification vs. Regression) and the specific dataset. Defaults can not be expected to work on all problems.
}
\seealso{
\link{elevate} for external cross-validation

Other Supervised Learning: \code{\link{s.ADABOOST}},
  \code{\link{s.ADDTREE}}, \code{\link{s.BART}},
  \code{\link{s.BAYESGLM}}, \code{\link{s.BRUTO}},
  \code{\link{s.C50}}, \code{\link{s.CART}},
  \code{\link{s.CTREE}}, \code{\link{s.DA}},
  \code{\link{s.ET}}, \code{\link{s.EVTREE}},
  \code{\link{s.GAM.default}}, \code{\link{s.GAM.formula}},
  \code{\link{s.GAMSEL}}, \code{\link{s.GAM}},
  \code{\link{s.GBM3}}, \code{\link{s.GBM}},
  \code{\link{s.GLMNET}}, \code{\link{s.GLM}},
  \code{\link{s.GLS}}, \code{\link{s.H2ODL}},
  \code{\link{s.H2OGBM}}, \code{\link{s.H2ORF}},
  \code{\link{s.IRF}}, \code{\link{s.KNN}},
  \code{\link{s.LDA}}, \code{\link{s.LM}},
  \code{\link{s.MARS}}, \code{\link{s.MLRF}},
  \code{\link{s.NBAYES}}, \code{\link{s.NLA}},
  \code{\link{s.NLS}}, \code{\link{s.NW}},
  \code{\link{s.POLYMARS}}, \code{\link{s.PPR}},
  \code{\link{s.PPTREE}}, \code{\link{s.QDA}},
  \code{\link{s.QRNN}}, \code{\link{s.RANGER}},
  \code{\link{s.RFSRC}}, \code{\link{s.RF}},
  \code{\link{s.SGD}}, \code{\link{s.SPLS}},
  \code{\link{s.SVM}}, \code{\link{s.TFN}},
  \code{\link{s.XGBLIN}}, \code{\link{s.XGB}}

Other Deep Learning: \code{\link{d.H2OAE}},
  \code{\link{p.MXINCEPTION}}, \code{\link{s.H2ODL}},
  \code{\link{s.TFN}}
}
\author{
Efstathios D. Gennatas
}
\concept{Deep Learning}
\concept{Supervised Learning}
