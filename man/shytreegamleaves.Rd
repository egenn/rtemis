% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/shytreegamleaves.R
\name{shytreegamleaves}
\alias{shytreegamleaves}
\title{\pkg{rtemis internal}: Low-level Stepwise Linear Additive Tree procedure}
\usage{
shytreegamleaves(
  x,
  y,
  x.valid = NULL,
  y.valid = NULL,
  type,
  lookback = FALSE,
  weights = NULL,
  max.leaves = 5,
  gamleaves = FALSE,
  gamlearner = "s_GAMSEL",
  gam.params = list(degrees = 5),
  learning.rate = 1,
  minobsinnode.lin = 10,
  lin.type = "glmnet",
  single.lin.type = "glmnet",
  gamma = 0.01,
  gamma.on.lin = FALSE,
  select.leaves.smooth = TRUE,
  alpha = 1,
  lambda = 0.01,
  lambda.seq = NULL,
  cv.glmnet.nfolds = 5,
  which.cv.glmnet.lambda = "lambda.1se",
  nvmax = 2,
  part.minsplit = 100,
  part.xval = 0,
  part.max.depth = 1,
  part.cp = 0,
  part.minbucket = 50,
  .rho = TRUE,
  rho.max = 1000,
  rho.def = 0.1,
  loss.fn = switch(type, Regression = mse, Classification = class.loss, Survival =
    surv.loss),
  verbose = TRUE,
  plot.tuning = TRUE,
  trace = 0
)
}
\arguments{
\item{x}{Data frame}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{lookback}{Logical: If TRUE, check validation error to decide best number of leaves to use.
Default = TRUE}

\item{weights}{Numeric vector: Weights for cases. For classification, \code{weights} takes precedence
over \code{ipw}, therefore set \code{weights = NULL} if using \code{ipw}.
Note: If \code{weight} are provided, \code{ipw} is not used. Leave NULL if setting \code{ipw = TRUE}. Default = NULL}

\item{max.leaves}{Integer: Total number of terminal nodes to reach.
1 is a special case where no split is performed and a linear model is trained.
Otherwise, this should be an even number as each split introduces two children nodes.
Note: this is total N of nodes in the tree, with the root uncounted,
not the number of terminal nodes.}

\item{lin.type}{Character: See \link{lincoef} for options}

\item{single.lin.type}{Character same options as \code{lin.type}, linear model to fit when
\code{max.leaves = 1}}

\item{gamma}{Numeric: Soft weighting parameter. Weights of cases that do not
belong to node get multiplied by this amount}

\item{lambda}{Float: lambda parameter for \code{MASS::lm.ridge} Default = .01}

\item{nvmax}{[gS] Integer: Number of max features to use for lin.type "allSubsets", "forwardStepwise", or
"backwardStepwise". If values greater than n of features in \code{x} are provided, they will be excluded}

\item{part.max.depth}{Integer: Max depth for each tree model within the additive tree}

\item{loss.fn}{Function with arguments \code{y, Fval }
Allows you to define a custom loss function. Defaults to \code{class.loss()} for classification
\code{mse()} for regression}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{plot.tuning}{Logical: If TRUE, plot validation error during gridsearch}

\item{trace}{Integer: If higher than 0, will print more information to the console. Default = 0}
}
\description{
Train a Linear Additive Tree for Classification & Regression
}
\details{
With \code{max.nodes = 0}, the output is a linear model trained according to \code{lin.type}
Note that lambda is treated differently by \code{glmnet::glmnet} and \code{MASS::lm.ridge}
}
\author{
E.D. Gennatas
}
\keyword{internal}
