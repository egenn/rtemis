% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train_cv.R
\name{train_cv}
\alias{train_cv}
\title{Tune, Train, and Test an \pkg{rtemis} Learner by Nested Resampling}
\usage{
train_cv(
  x,
  y = NULL,
  alg = "ranger",
  train.params = list(),
  .preprocess = NULL,
  .decompose = NULL,
  weights = NULL,
  n.repeats = 1,
  outer.resampling = setup.resample(resampler = "strat.sub", n.resamples = 10),
  inner.resampling = setup.resample(resampler = "kfold", n.resamples = 5),
  bag.fn = median,
  x.name = NULL,
  y.name = NULL,
  save.mods = TRUE,
  save.tune = TRUE,
  bag.fitted = FALSE,
  outer.n.workers = 1,
  print.plot = FALSE,
  plot.fitted = FALSE,
  plot.predicted = TRUE,
  plot.theme = rtTheme,
  print.res.plot = FALSE,
  question = NULL,
  verbose = TRUE,
  res.verbose = FALSE,
  trace = 0,
  headless = FALSE,
  outdir = NULL,
  save.plots = FALSE,
  save.rt = ifelse(!is.null(outdir), TRUE, FALSE),
  save.mod = TRUE,
  save.res = FALSE,
  debug = FALSE,
  ...
)
}
\arguments{
\item{x}{Numeric vector or matrix / data frame of features i.e. independent variables}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{alg}{Character: Learner to use. Options: \link{select_learn}}

\item{train.params}{Optional named list of parameters to be passed to
\code{alg}. All parameters can be passed as part of \code{...} as well}

\item{.preprocess}{Optional named list of parameters to be passed to
\link{preprocess}. Set using \link{setup.preprocess},
e.g. \code{decom = setup.preprocess(impute = TRUE)}}

\item{.decompose}{Optional named list of parameters to be used for
decomposition / dimensionality reduction. Set using \link{setup.decompose},
e.g. \code{decom = setup.decompose("ica", 12)}}

\item{weights}{Numeric vector: Weights for cases. For classification, \code{weights} takes precedence
over \code{ifw}, therefore set \code{weights = NULL} if using \code{ifw}.
Note: If \code{weight} are provided, \code{ifw} is not used. Leave NULL if setting \code{ifw = TRUE}.}

\item{n.repeats}{Integer: Number of times to repeat the outer resampling.
This was added for completeness, but in practice we use either k-fold
crossvalidation, e.g. 10-fold, especially in large samples, or a higher number of
stratified subsamples, e.g. 25, for smaller samples}

\item{outer.resampling}{List: Output of \link{setup.resample} to define outer
resampling scheme}

\item{inner.resampling}{List: Output of \link{setup.resample} to define inner
resampling scheme}

\item{bag.fn}{Function to use to average prediction if
\code{bag.fitted = TRUE}. Default = \code{median}}

\item{x.name}{Character: Name of predictor dataset}

\item{y.name}{Character: Name of outcome}

\item{save.mods}{Logical: If TRUE, retain trained models in object,
otherwise discard (save space if running many resamples).}

\item{save.tune}{Logical: If TRUE, save the best.tune data frame for each
resample (output of gridSearchLearn)}

\item{bag.fitted}{Logical: If TRUE, use all models to also get a bagged
prediction on the full sample. To get a bagged prediction on new data using
the same models, use \link{predict.rtModCV}}

\item{outer.n.workers}{Integer: Number of cores to use for the outer i.e.
testing resamples. You are likely parallelizing either in the inner
(tuning) or the learner itself is parallelized. Don't parallelize the
parallelization}

\item{print.plot}{Logical: if TRUE, produce plot using \code{mplot3}
Takes precedence over \code{plot.fitted} and \code{plot.predicted}.}

\item{plot.fitted}{Logical: if TRUE, plot True (y) vs Fitted}

\item{plot.predicted}{Logical: if TRUE, plot True (y.test) vs Predicted.
Requires \code{x.test} and \code{y.test}}

\item{plot.theme}{Character: "zero", "dark", "box", "darkbox"}

\item{print.res.plot}{Logical: Print model performance plot for each
resample.
from all resamples. Defaults to TRUE}

\item{question}{Character: the question you are attempting to answer with this model, in plain language.}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{res.verbose}{Logical: Passed to each individual learner's \code{verbose} argument}

\item{trace}{Integer: (Not really used) Print additional information if > 0.}

\item{headless}{Logical: If TRUE, turn off all plotting.}

\item{outdir}{Character: Path where output should be saved}

\item{save.plots}{Logical: If TRUE, save plots to outdir}

\item{save.rt}{Logical: If TRUE and \code{outdir} is set, save all models to
\code{outdir}}

\item{save.mod}{Logical: If TRUE, save all output to an RDS file in \code{outdir}
\code{save.mod} is TRUE by default if an \code{outdir} is defined. If set to TRUE, and no \code{outdir}
is defined, outdir defaults to \code{paste0("./s.", mod.name)}}

\item{save.res}{Logical: If TRUE, save the full output of each model trained
on differents resamples under subdirectories of \code{outdir}}

\item{debug}{Logical: If TRUE, sets \code{outer.n.workers} to 1, \code{options(error=recover)},
and options(warn = 2)}

\item{...}{Additional train.params to be passed to learner. Will be concatenated
with \code{train.params}}
}
\value{
Object of class \code{rtModCV} (Regression) or
\code{rtModCVClass} (Classification)
\item{error.test.repeats}{the mean or aggregate error, as appropriate, for each repeat}
\item{error.test.repeats.mean}{the mean error of all repeats, i.e. the mean of \code{error.test.repeats}}
\item{error.test.repeats.sd}{if \code{n.repeats} > 1, the standard deviation of \code{error.test.repeats}}
\item{error.test.res}{the error for each resample, for each repeat}
}
\description{
\code{train} is a high-level function to tune, train, and test an
\pkg{rtemis} model by nested resampling, with optional preprocessing and
decomposition of input features
}
\details{
\itemize{
\item Note on resampling: You should never use an outer resampling method with
replacement if you will also be using an inner resampling (for tuning).
The duplicated cases from the outer resampling may appear both in the
training and testing sets of the inner resamples, leading to underestimated
testing error.
\item If there is an error while running either the outer or inner resamples in
parallel, the error message returned by R will likely be unhelpful. Repeat
the command after setting both inner and outer resample run to use a single
core, which should provide an informative message.
}

The \code{train} command is replacing \code{elevate}.
Note: specifying id.strat for the inner resampling is not yet supported.
}
\examples{
\dontrun{
# Regression

x <- rnormmat(100, 50)
w <- rnorm(50)
y <- x \%*\% w + rnorm(50)
mod <- train(x, y)

# Classification

data(Sonar, package = "mlbench")
mod <- train(Sonar)
}
}
\author{
E.D. Gennatas
}
