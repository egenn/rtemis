% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/s.XGB.R
\name{s.XGB}
\alias{s.XGB}
\title{XGboost Classification and Regression [C, R]}
\usage{
s.XGB(
  x,
  y = NULL,
  x.test = NULL,
  y.test = NULL,
  x.name = NULL,
  y.name = NULL,
  booster = c("gbtree", "gblinear", "dart"),
  silent = 1,
  missing = NA,
  nrounds = 500L,
  force.nrounds = NULL,
  weights = NULL,
  ipw = TRUE,
  ipw.type = 2,
  upsample = FALSE,
  downsample = FALSE,
  resample.seed = NULL,
  obj = NULL,
  feval = NULL,
  maximize = NULL,
  xgb.verbose = NULL,
  print_every_n = 100L,
  early.stopping.rounds = 50L,
  eta = 0.1,
  gamma = 0,
  max.depth = 3,
  min.child.weight = 5,
  max.delta.step = 0,
  subsample = 0.75,
  colsample.bytree = NULL,
  colsample.bylevel = 1,
  lambda = NULL,
  lambda.bias = 0,
  alpha = 0,
  tree.method = "auto",
  sketch.eps = 0.03,
  num.parallel.tree = 1,
  base.score = NULL,
  objective = NULL,
  sample.type = "uniform",
  normalize.type = "forest",
  rate.drop = 0.1,
  skip.drop = 0.5,
  resampler = "strat.sub",
  n.resamples = 10,
  train.p = 0.75,
  strat.n.bins = 4,
  stratify.var = NULL,
  target.length = NULL,
  seed = NULL,
  error.curve = FALSE,
  plot.res = TRUE,
  save.res = FALSE,
  save.res.mod = FALSE,
  importance = FALSE,
  print.plot = TRUE,
  plot.fitted = NULL,
  plot.predicted = NULL,
  plot.theme = getOption("rt.fit.theme", "lightgrid"),
  question = NULL,
  rtclass = NULL,
  save.dump = FALSE,
  verbose = TRUE,
  n.cores = 1,
  nthread = NULL,
  parallel.type = c("psock", "fork"),
  outdir = NULL,
  save.mod = ifelse(!is.null(outdir), TRUE, FALSE)
)
}
\arguments{
\item{x}{Numeric vector or matrix / data frame of features i.e. independent variables}

\item{y}{Numeric vector of outcome, i.e. dependent variable}

\item{x.test}{Numeric vector or matrix / data frame of testing set features
Columns must correspond to columns in \code{x}}

\item{y.test}{Numeric vector of testing set outcome}

\item{x.name}{Character: Name for feature set}

\item{y.name}{Character: Name for outcome}

\item{booster}{Character: Booster to use. Options: "gbtree", "gblinear"}

\item{silent}{0: print XGBoost messages; 1: print no XGBoost messages}

\item{missing}{String or Numeric: Which values to consider as missing. Default = NA}

\item{nrounds}{Integer: Maximum number of rounds to run. Can be set to a high number as early stopping
will limit nrounds by monitoring inner CV error}

\item{force.nrounds}{Integer: Number of rounds to run if not estimating optimal number by CV}

\item{weights}{Numeric vector: Weights for cases. For classification, \code{weights} takes precedence
over \code{ipw}, therefore set \code{weights = NULL} if using \code{ipw}.
Note: If \code{weight} are provided, \code{ipw} is not used. Leave NULL if setting \code{ipw = TRUE}. Default = NULL}

\item{ipw}{Logical: If TRUE, apply inverse probability weighting (for Classification only).
Note: If \code{weights} are provided, \code{ipw} is not used. Default = TRUE}

\item{ipw.type}{Integer {0, 1, 2}
1: class.weights as in 0, divided by max(class.weights)
2: class.weights as in 0, divided by min(class.weights)
Default = 2}

\item{upsample}{Logical: If TRUE, upsample cases to balance outcome classes (for Classification only)
Caution: upsample will randomly sample with replacement if the length of the majority class is more than double
the length of the class you are upsampling, thereby introducing randomness}

\item{resample.seed}{Integer: If provided, will be used to set the seed during upsampling.
Default = NULL (random seed)}

\item{obj}{Function: Custom objective function. See \code{?xgboost::xgboost}}

\item{feval}{Function: Custom evaluation function. See \code{?xgboost::xgboost}}

\item{xgb.verbose}{Integer: Verbose level for XGB learners used for tuning.}

\item{print_every_n}{Integer: Print evaluation metrics every this many iterations}

\item{early.stopping.rounds}{Integer: Training on resamples of \code{x.train} (tuning) will stop if performance
does not improve for this many rounds}

\item{eta}{[gS] Float (0, 1): Learning rate. Default = .1}

\item{gamma}{[gS] Float: Minimum loss reduction required to make further partition}

\item{max.depth}{[gS] Integer: Maximum tree depth. (Default = 6)}

\item{subsample}{[gS] Float:}

\item{colsample.bytree}{[gS]}

\item{colsample.bylevel}{[gS]}

\item{lambda}{[gS] L2 regularization on weights}

\item{alpha}{[gS] L1 regularization on weights}

\item{tree.method}{[gS] XGBoost tree construction algorithm (Default = "auto")}

\item{sketch.eps}{[gS] Float (0, 1):}

\item{num.parallel.tree}{Integer: N of trees to grow in parallel: Results in Random Forest -like algorithm.
(Default = 1; i.e. regular boosting)}

\item{base.score}{Float: The mean outcome response (no need to set)}

\item{objective}{(Default = NULL)}

\item{sample.type}{(Default = "uniform")}

\item{normalize.type}{(Default = "forest")}

\item{print.plot}{Logical: if TRUE, produce plot using \code{mplot3}
Takes precedence over \code{plot.fitted} and \code{plot.predicted}. Default = TRUE}

\item{plot.fitted}{Logical: if TRUE, plot True (y) vs Fitted}

\item{plot.predicted}{Logical: if TRUE, plot True (y.test) vs Predicted.
Requires \code{x.test} and \code{y.test}}

\item{plot.theme}{Character: "zero", "dark", "box", "darkbox"}

\item{question}{Character: the question you are attempting to answer with this model, in plain language.}

\item{rtclass}{Character: Class type to use. "S3", "S4", "RC", "R6"}

\item{verbose}{Logical: If TRUE, print summary to screen.}

\item{nthread}{Integer: Number of threads for xgboost using OpenMP. Only parallelize resamples
using \code{n.cores} or the xgboost execution using this setting. At the moment of writing, parallelization via this
parameter causes a linear booster to fail most of the times. Therefore, default is rtCores
for 'gbtree', 1 for 'gblinear'}

\item{outdir}{Path to output directory.
If defined, will save Predicted vs. True plot, if available,
as well as full model output, if \code{save.mod} is TRUE}

\item{save.mod}{Logical: If TRUE, save all output to an RDS file in \code{outdir}
\code{save.mod} is TRUE by default if an \code{outdir} is defined. If set to TRUE, and no \code{outdir}
is defined, outdir defaults to \code{paste0("./s.", mod.name)}}

\item{lambda_bias}{[gS] for *linear* booster: L2 regularization on bias}
}
\value{
\link{rtMod} object
}
\description{
Tune hyperparameters using grid search and resampling,
train a final model, and validate it
}
\details{
[gS]: indicates parameter will be autotuned by grid search if multiple values are passed.
(s.XGB does its own grid search, similar to gridSearchLearn, may switch to gridSearchLearn similar to s.GBM)
Learn more about XGboost's parameters here: http://xgboost.readthedocs.io/en/latest/parameter.html
Case weights and therefore IPW do not seem to work, despite following documentation.
See how ipw = T fails and upsample = T works in imbalanced dataset.
11.24.16: Updated to work with latest development version of XGBoost from github, which changed some of
\code{xgboost}'s return values and is therefore not compatible with older versions
\link{s.XGBLIN} is a wrapper for \code{s.XGB} with \code{booster = "gblinear"}
}
\seealso{
\link{elevate} for external cross-validation

Other Supervised Learning: 
\code{\link{s.ADABOOST}()},
\code{\link{s.ADDTREE}()},
\code{\link{s.BART}()},
\code{\link{s.BAYESGLM}()},
\code{\link{s.BRUTO}()},
\code{\link{s.C50}()},
\code{\link{s.CART}()},
\code{\link{s.CTREE}()},
\code{\link{s.DA}()},
\code{\link{s.ET}()},
\code{\link{s.EVTREE}()},
\code{\link{s.GAM.default}()},
\code{\link{s.GAM.formula}()},
\code{\link{s.GAMSELX2}()},
\code{\link{s.GAMSELX}()},
\code{\link{s.GAMSEL}()},
\code{\link{s.GAM}()},
\code{\link{s.GBM3}()},
\code{\link{s.GBM}()},
\code{\link{s.GLMNET}()},
\code{\link{s.GLM}()},
\code{\link{s.GLS}()},
\code{\link{s.H2ODL}()},
\code{\link{s.H2OGBM}()},
\code{\link{s.H2ORF}()},
\code{\link{s.IRF}()},
\code{\link{s.KNN}()},
\code{\link{s.LDA}()},
\code{\link{s.LM}()},
\code{\link{s.MARS}()},
\code{\link{s.MLRF}()},
\code{\link{s.NBAYES}()},
\code{\link{s.NLA}()},
\code{\link{s.NLS}()},
\code{\link{s.NW}()},
\code{\link{s.POLYMARS}()},
\code{\link{s.PPR}()},
\code{\link{s.PPTREE}()},
\code{\link{s.QDA}()},
\code{\link{s.QRNN}()},
\code{\link{s.RANGER}()},
\code{\link{s.RFSRC}()},
\code{\link{s.RF}()},
\code{\link{s.SGD}()},
\code{\link{s.SPLS}()},
\code{\link{s.SVM}()},
\code{\link{s.TFN}()},
\code{\link{s.XGBLIN}()}

Other Tree-based methods: 
\code{\link{s.ADABOOST}()},
\code{\link{s.ADDTREE}()},
\code{\link{s.BART}()},
\code{\link{s.C50}()},
\code{\link{s.CART}()},
\code{\link{s.CTREE}()},
\code{\link{s.ET}()},
\code{\link{s.EVTREE}()},
\code{\link{s.GBM3}()},
\code{\link{s.GBM}()},
\code{\link{s.H2OGBM}()},
\code{\link{s.H2ORF}()},
\code{\link{s.IRF}()},
\code{\link{s.MLRF}()},
\code{\link{s.PPTREE}()},
\code{\link{s.RANGER}()},
\code{\link{s.RFSRC}()},
\code{\link{s.RF}()}
}
\author{
E.D. Gennatas
}
\concept{Supervised Learning}
\concept{Tree-based methods}
